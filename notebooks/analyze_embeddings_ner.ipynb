{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea15ab9e-433d-4593-9a67-d167ce71909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/tu/tu_tu/tu_kmpaj01/miniforge3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/pfs/data5/home/tu/tu_tu/tu_kmpaj01/miniforge3/envs/torch/lib/python3.12/site-packages/numba/np/ufunc/dufunc.py:343: NumbaWarning: \u001b[1mCompilation requested for previously compiled argument types ((uint32,)). This has no effect and perhaps indicates a bug in the calling code (compiling a ufunc more than once for the same signature\u001b[0m\n",
      "  warnings.warn(msg, errors.NumbaWarning)\n",
      "/pfs/data5/home/tu/tu_tu/tu_kmpaj01/miniforge3/envs/torch/lib/python3.12/site-packages/numba/np/ufunc/dufunc.py:343: NumbaWarning: \u001b[1mCompilation requested for previously compiled argument types ((uint32,)). This has no effect and perhaps indicates a bug in the calling code (compiling a ufunc more than once for the same signature\u001b[0m\n",
      "  warnings.warn(msg, errors.NumbaWarning)\n",
      "/pfs/data5/home/tu/tu_tu/tu_kmpaj01/miniforge3/envs/torch/lib/python3.12/site-packages/numba/np/ufunc/dufunc.py:343: NumbaWarning: \u001b[1mCompilation requested for previously compiled argument types ((uint32,)). This has no effect and perhaps indicates a bug in the calling code (compiling a ufunc more than once for the same signature\u001b[0m\n",
      "  warnings.warn(msg, errors.NumbaWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "import umap.plot\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonlines\n",
    "import matplotlib.cm as cm\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85563c85-799e-4826-a06a-17b5eb72adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_arrays(arrays):\n",
    "  \"\"\"Pads all arrays in a list to the same length with 0s.\n",
    "  \"\"\"  \n",
    "  max_len = max(len(arr) for arr in arrays)\n",
    "  padded_arrays = [np.pad(arr, (0, max_len - len(arr)), mode='constant', constant_values=0) for arr in arrays]\n",
    "  return padded_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585d6837-c30e-4bc7-a9a1-69edc03b18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(arrays):\n",
    "  \"\"\"Pads a list of numpy arrays to the same shape.\n",
    "\n",
    "  Args:\n",
    "    arrays: A list of NumPy arrays. All arrays must have the same\n",
    "      dimensionality (rank).\n",
    "\n",
    "  Returns:\n",
    "    A list of padded NumPy arrays. The arrays will all have the same\n",
    "    shape as the array with the largest shape in the original list.\n",
    "  \"\"\"\n",
    "  max_len = np.max([arr.shape[0] for arr in arrays])\n",
    "  padded_arrays = []\n",
    "  for arr in arrays:\n",
    "    padding_len = max_len - arr.shape[0]\n",
    "    padded_array = np.pad(arr, ((padding_len, 0), (0, 0)), mode='constant')\n",
    "    padded_arrays.append(padded_array)\n",
    "  return padded_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc88630",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initial tests (do not run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f7794b-bc24-44f2-aea8-227e60c83335",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../NER_output/STRAIN\"\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, model_max_length=512)\n",
    "# model = AutoModel.from_pretrained(path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2161277-7bd4-47ba-8f78-bd4669d07017",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../NER/STRAIN/train.jsonls\") as f:\n",
    "    data = json.load(f)\n",
    "sentences = []\n",
    "for s in data:\n",
    "    sen = s[\"data\"][\"text\"]\n",
    "    sentences.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4737188-48b2-4165-9ac2-8c8af9f266c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to extract embeddings\n",
    "model.eval()  # Set model to evaluation mode\n",
    "embeddings = []\n",
    "\n",
    "# Process the dataset and extract embeddings\n",
    "with torch.no_grad():\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract embeddings from the desired layer (here, the last hidden state)\n",
    "        embeddings.append(outputs) # .mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf20e7-471a-4c24-8cbb-11b584e252bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ee6b2-ce8a-4c8b-8231-a860879d7667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare to extract embeddings\n",
    "model.eval()  # Set model to evaluation mode\n",
    "embeddings = []\n",
    "\n",
    "# Process the dataset and extract embeddings\n",
    "with torch.no_grad():\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract embeddings from the desired layer (here, the last hidden state)\n",
    "        embeddings.append(outputs.last_hidden_state[0].numpy()) # .mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62a842-2578-4e44-b258-720d01461bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7eb484-8095-46b5-ad4d-de5c8f79ba5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit = umap.UMAP()\n",
    "u = fit.fit_transform(dembeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ba8af-ff83-4b06-a2d7-cd2f8d4835a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Do it instead with the feature extraction method (do not run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44dba5-6526-4337-8852-45590b144c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = \"STRAIN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb932e0-a7b7-47d2-9504-b8284559488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= f\"../NER_output/{ner}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, model_max_length=512)\n",
    "model = AutoModelForTokenClassification.from_pretrained(path)\n",
    "\n",
    "fe = pipeline(task='feature-extraction', model=model, tokenizer=tokenizer,\n",
    "               aggregation_strategy=\"average\", device=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811621c-dfc0-4ed2-8701-498298faf370",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for sentence in sentences:\n",
    "    features.append(fe(sentence)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee85bb4-89fc-4091-9d15-c9ae5d0f46b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flat = [np.array(i).flatten() for i in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d3c64-3f23-430b-9ae4-e049c1350d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_pad = pad_arrays(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80493a10-c9fe-4509-b9ce-6ef779f06495",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = umap.UMAP()\n",
    "u = fit.fit_transform(flat_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac50e80-9816-4889-9d25-11877066ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(u[:,0],u[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9926c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Do all models at once (do not run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a2b28a-9002-47f5-a892-76fe515ff387",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "# Load models and tokenizers outside the loop to avoid reloading for each NER\n",
    "loaded_models_tokenizers = {\n",
    "    ner: (\n",
    "        AutoTokenizer.from_pretrained(f\"../NER_output/{ner}\", model_max_length=512),\n",
    "        AutoModelForTokenClassification.from_pretrained(f\"../NER_output/{ner}\")\n",
    "    ) for ner in ners\n",
    "}\n",
    "\n",
    "for ner in tqdm(ners, desc=\"Processing NER models\"):\n",
    "    with open(f\"../NER/{ner}/test.jsonls\") as f:\n",
    "        data = json.load(f)\n",
    "    sentences = [s[\"data\"][\"text\"] for s in data]\n",
    "    \n",
    "    tokenizer, model = loaded_models_tokenizers[ner]\n",
    "    \n",
    "    fe = pipeline(task='feature-extraction', model=model, tokenizer=tokenizer,\n",
    "                   aggregation_strategy=\"average\", device=0)\n",
    "    features = [np.array(fe(sentence)[0]) for sentence in sentences]\n",
    "    \n",
    "    flat = [feature.flatten() for feature in features]\n",
    "    flat_pad = pad_arrays(flat)\n",
    "    df = pd.DataFrame(flat_pad)\n",
    "    df[\"ner\"] = ner\n",
    "    l.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d19f08-4526-47ef-9aa6-939970b6dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.concat(l,axis=0)\n",
    "dfs = dfs.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0703652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a UMAP object and fit it\n",
    "mapper = umap.UMAP(metric=\"cosine\").fit(dfs.iloc[:, :-1])\n",
    "\n",
    "# Get the labels from the last column of the dataframe\n",
    "labels = dfs.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa095c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot using UMAP's plotting API, mapping labels to colors\n",
    "p = umap.plot.points(mapper, labels=labels, theme='fire', width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5876a-890f-4416-8215-4ce9841a07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and tokenizers outside the loop to avoid reloading for each NER\n",
    "loaded_models_tokenizers = {\n",
    "    ner: (\n",
    "        AutoTokenizer.from_pretrained(f\"../NER_output/{ner}\", model_max_length=512),\n",
    "        AutoModelForTokenClassification.from_pretrained(f\"../NER_output/{ner}\")\n",
    "    ) for ner in ners\n",
    "}\n",
    "\n",
    "# Initialize UMAP and figure for plotting\n",
    "fit = umap.UMAP(metric=\"cosine\")\n",
    "plt.figure()\n",
    "\n",
    "colors = iter(plt.cm.rainbow(np.linspace(0, 1, len(ners))))\n",
    "\n",
    "for ner in tqdm(ners, desc=\"Processing NER models\"):\n",
    "    with open(f\"../NER/{ner}/test.jsonls\") as f:\n",
    "        data = json.load(f)\n",
    "    sentences = [s[\"data\"][\"text\"] for s in data]\n",
    "    \n",
    "    tokenizer, model = loaded_models_tokenizers[ner]\n",
    "    \n",
    "    fe = pipeline(task='feature-extraction', model=model, tokenizer=tokenizer,\n",
    "                   aggregation_strategy=\"average\", device=0)\n",
    "    features = [np.array(fe(sentence)[0]) for sentence in sentences]\n",
    "    \n",
    "    flat = [feature.flatten() for feature in features]\n",
    "    flat_pad = pad_arrays(flat)\n",
    "    \n",
    "    # Apply UMAP to the result of each model\n",
    "    u = fit.fit_transform(flat_pad)\n",
    "    \n",
    "    # Plot the result for each model on the same axes with different colors\n",
    "    plt.scatter(u[:,0], u[:,1], color=next(colors), label=ner,s=1)\n",
    "\n",
    "plt.title(\"UMAP projection for all NER models\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810602d4",
   "metadata": {},
   "source": [
    "## NER (run from here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36a2b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ners =[\"COMPOUND\"]#, \"DISEASE\", \"EFFECT\",\"ISOLATE\",\"MEDIUM\",\"ORGANISM\",\"PHENOTYPE\",\"STRAIN\",\"SPECIES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c096d6c-59dc-41f5-ac6e-1d8fec809710",
   "metadata": {},
   "source": [
    "output_attentions=True, https://github.com/explosion/spaCy/issues/7283\n",
    "\n",
    "\n",
    "get the output attention matrix and use it to weight the tokens instead of just doing the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6177790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing NER models:   0%|          | 0/1 [00:00<?, ?it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing NER models: 100%|██████████| 1/1 [00:46<00:00, 46.69s/it]\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "# Load models and tokenizers outside the loop to avoid reloading for each NER\n",
    "loaded_models_tokenizers = {\n",
    "    ner: (\n",
    "        AutoTokenizer.from_pretrained(f\"../NER_output/{ner}\", model_max_length=512),\n",
    "        AutoModelForTokenClassification.from_pretrained(f\"../NER_output/{ner}\",output_attentions=True,output_hidden_states=True)\n",
    "    ) for ner in ners\n",
    "}\n",
    "\n",
    "for ner in tqdm(ners, desc=\"Processing NER models\"):\n",
    "    data = []\n",
    "    for set in [\"train\",\"test\",\"dev\"]:        \n",
    "        with open(f\"../NER/{ner}/{set}.jsonls\") as f:\n",
    "            data+=json.load(f)\n",
    "    sentences = [s[\"data\"][\"text\"] for s in data]\n",
    "    \n",
    "    tokenizer, model = loaded_models_tokenizers[ner]\n",
    "    \n",
    "    fe = pipeline(task='feature-extraction', model=model, tokenizer=tokenizer,\n",
    "                   aggregation_strategy=\"average\", device=0)\n",
    "    features = [np.array(fe(sentence)[0]).mean(axis=0) for sentence in sentences]\n",
    "    \n",
    "    # flat = [feature.flatten() for feature in features]\n",
    "    # flat_pad = pad_arrays(flat)\n",
    "    df = pd.DataFrame(features)\n",
    "    df[\"ner\"] = ner\n",
    "    l.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75790303-2478-41e5-a4ca-c34e2fe71efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.88205902, -2.3864504 ,  4.26617918])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(fe(sentences[0])[0]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9caae28f-d31b-457a-bafc-6e502296042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = AutoTokenizer.from_pretrained(f\"../NER_output/{ner}\", model_max_length=512),\n",
    "model1 = AutoModelForTokenClassification.from_pretrained(f\"../NER_output/{ner}\",output_attentions=True,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a785075-dd5b-4b88-997d-3515902d3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer1[0].encode(sentences[0],return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "341b6d08-57f7-4aed-80a6-b6d33036bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model1(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6168b32-61d1-4035-80cb-7dbe01cf978e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 42, 1024])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce75f794-6a0c-4f74-a6bb-707a319d5f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8821, -2.3864,  4.2662], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f61ea7-2f8f-40f8-8ceb-08eb58a5f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = output.to_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3243a838-1d9a-46e8-ad62-bc7ea7f58561",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tuples[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476613b-449c-485a-9915-4102ce8f644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights_mean = output.attentions[-1].mean(dim=1)\n",
    "\n",
    "attention_weights_mean = attention_weights_mean[:, :]\n",
    "embeddings = torch.Tensor(embeddings.detach().numpy()[ :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2d1e4-56e9-479f-8916-beb7183356b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d8a4c-35ec-4d42-8164-2ac865eb5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95512a-bbe1-4873-ae5c-363a2c5adb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dce39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.concat(l,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a UMAP object and fit it\n",
    "mapper = umap.UMAP(n_neighbors=20,min_dist=0.05).fit(dfs.iloc[:, :-1])\n",
    "#mapper = umap.UMAP().fit(dfs.iloc[:, :-1])\n",
    "\n",
    "# Get the labels from the last column of the dataframe\n",
    "labels = dfs.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc859a67-b0a4-4517-b12d-e5d36f9fc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ddb72c-5a1d-4f74-ad45-c097e67e71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a colormap with enough colors\n",
    "cmap = cm.get_cmap('Set3', len(labels.unique()))\n",
    "\n",
    "# Create a dictionary mapping labels to colors\n",
    "label_color_mapping = {label: cmap(i) for i, label in enumerate(labels.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd91c4-1c4e-4d27-94d5-c120eca0925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_color_mapping = {'COMPOUND': (0.5529411764705883, 0.8274509803921568, 0.7803921568627451, 1.0),\n",
    " 'DISEASE': (1.0, 1.0, 0.7019607843137254, 1.0),\n",
    " 'EFFECT': (0.984313725490196, 0.5019607843137255, 0.4470588235294118, 1.0),\n",
    " 'ISOLATE': (0.5019607843137255, 0.6941176470588235, 0.8274509803921568, 1.0),\n",
    " 'MEDIUM': (0.7019607843137254, 0.8705882352941177, 0.4117647058823529, 1.0),\n",
    " 'ORGANISM': (0.9882352941176471, 0.803921568627451, 0.8980392156862745, 1.0),\n",
    " 'PHENOTYPE': (0.7372549019607844,0.5019607843137255,0.7411764705882353,1.0),\n",
    " 'STRAIN': (1.0, 0.9294117647058824, 0.43529411764705883, 1.0),\n",
    " 'SPECIES': (0.8, 0.9215686274509803, 0.7725490196078432, 1.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c19816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot using UMAP's plotting API, mapping labels to colors\n",
    "p = umap.plot.points(mapper, labels=labels, color_key=label_color_mapping,width=700, height=700)\n",
    "plt.savefig(\"figures/predictions/umap_ner.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93dcf7",
   "metadata": {},
   "source": [
    "## RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004120a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = config[\"rel_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c218684",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "loaded_models_tokenizers = {\n",
    "    rel: (\n",
    "        AutoTokenizer.from_pretrained(f\"../REL_output/{rel}\"),\n",
    "        AutoModelForTokenClassification.from_pretrained(f\"../REL_output/{rel}\")\n",
    "    ) for rel in rels\n",
    "}\n",
    "\n",
    "for rel in tqdm(rels, desc=\"Processing REL models\"):\n",
    "    data = []\n",
    "    for set in [\"train\",\"test\",\"dev\"]:        \n",
    "        with jsonlines.open(f\"../REL/{rel}/{set}.json\") as reader:\n",
    "            data += [obj for obj in reader]\n",
    "    sentences = [s[\"sentence\"] for s in data]\n",
    "    \n",
    "    tokenizer, model = loaded_models_tokenizers[rel]\n",
    "    \n",
    "    fe = pipeline(task='feature-extraction', model=model, tokenizer=tokenizer,\n",
    "                   aggregation_strategy=\"average\", device=0)\n",
    "    features = [np.array(fe(sentence)[0]).mean(axis=0) for sentence in sentences]\n",
    "\n",
    "    df = pd.DataFrame(features)\n",
    "    df[\"rel\"] = rel\n",
    "    l.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.concat(l,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8591e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[\"non_strain_ner\"] = dfs[\"rel\"].str.split(\":\",expand=True)[0].str.split(\"-\").apply(lambda x: [i for i in x if i != \"STRAIN\"]).apply(lambda x: x[0] if len(x) == 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(14, 7), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "unique_non_strain_ner = dfs[\"non_strain_ner\"].unique()\n",
    "\n",
    "for i, non_strain_ner in enumerate(unique_non_strain_ner):\n",
    "    if i >= 12:  # Only plot the first 12 to fit the grid\n",
    "        break\n",
    "    # Filter the dataframe for the current non_strain_ner\n",
    "    df_filtered = dfs[dfs[\"non_strain_ner\"] == non_strain_ner]\n",
    "    \n",
    "    # Create a UMAP object and fit it\n",
    "    mapper = umap.UMAP(n_neighbors=20, min_dist=0.05).fit(df_filtered.iloc[:, :-2])  # Exclude the last two columns (rel and non_strain_ner)\n",
    "    \n",
    "    # Get the labels from the 'rel' column\n",
    "    labels = df_filtered[\"rel\"]\n",
    "    \n",
    "    # Plot using UMAP's plotting API, mapping labels to colors\n",
    "    umap.plot.points(mapper, labels=labels, color_key_cmap='Accent', ax=axes[i])\n",
    "    axes[i].set_title(non_strain_ner)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/predictions/umap_rel_grouped_entity.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[\"relation\"] = dfs.rel.str.split(\":\",expand=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae4542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "unique_relation = dfs[\"relation\"].unique()\n",
    "\n",
    "for i, relation in enumerate(unique_relation):\n",
    "    if i >= 12:  # Only plot the first 12 to fit the grid\n",
    "        break\n",
    "    # Filter the dataframe for the current relation\n",
    "    df_filtered = dfs[dfs[\"relation\"] == relation]\n",
    "    \n",
    "    # Create a UMAP object and fit it\n",
    "    mapper = umap.UMAP(n_neighbors=20, min_dist=0.05).fit(df_filtered.iloc[:, :-3])  # Exclude the last two columns (rel and relation)\n",
    "    \n",
    "    # Get the labels from the 'rel' column\n",
    "    labels = df_filtered[\"rel\"]\n",
    "    \n",
    "    # Plot using UMAP's plotting API, mapping labels to colors\n",
    "    umap.plot.points(mapper, labels=labels, color_key_cmap='Accent', ax=axes[i])\n",
    "    axes[i].set_title(relation)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/predictions/umap_rel_grouped_relation.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecbfe5-86c7-48f6-89b8-2cfa164f8a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017dd6e5-fc9b-41de-8b57-45c7cc235df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
